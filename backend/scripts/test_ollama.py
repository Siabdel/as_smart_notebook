#!/usr/bin/env python3
"""
Script de test pour v√©rifier la connexion Ollama et la g√©n√©ration d'embeddings.
Usage: python scripts/test_ollama.py
"""

import sys
import httpx
import json
import time
from typing import List, Dict, Any


# Configuration
OLLAMA_BASE_URL = "http://localhost:11434"
EMBEDDING_MODEL = "nomic-embed-text"


def print_header(text: str):
    """Affiche un header color√©."""
    print("\n" + "="*60)
    print(f"  {text}")
    print("="*60)


def test_ollama_connection() -> bool:
    """
    Teste la connexion au serveur Ollama.
    """
    print_header("Test 1: Connexion au serveur Ollama")
    
    try:
        url = f"{OLLAMA_BASE_URL}/api/tags"
        response = httpx.get(url, timeout=5.0)
        response.raise_for_status()
        
        data = response.json()
        models = data.get('models', [])
        
        print(f"‚úÖ Connexion r√©ussie √† Ollama ({OLLAMA_BASE_URL})")
        print(f"üì¶ {len(models)} mod√®le(s) disponible(s):")
        
        for model in models:
            name = model.get('name', 'Unknown')
            size = model.get('size', 0) / (1024**3)  # Conversion en GB
            print(f"   - {name} ({size:.2f} GB)")
        
        return True
    
    except httpx.ConnectError:
        print(f"‚ùå Impossible de se connecter √† Ollama sur {OLLAMA_BASE_URL}")
        print("   V√©rifiez qu'Ollama est d√©marr√©:")
        print("   sudo systemctl status ollama")
        print("   ou")
        print("   ollama serve")
        return False
    
    except Exception as e:
        print(f"‚ùå Erreur: {str(e)}")
        return False


def check_model_availability(model_name: str) -> bool:
    """
    V√©rifie si un mod√®le sp√©cifique est disponible.
    """
    print_header(f"Test 2: V√©rification du mod√®le {model_name}")
    
    try:
        url = f"{OLLAMA_BASE_URL}/api/tags"
        response = httpx.get(url, timeout=5.0)
        response.raise_for_status()
        
        data = response.json()
        models = data.get('models', [])
        model_names = [m.get('name', '') for m in models]
        
        if model_name in model_names:
            print(f"‚úÖ Mod√®le '{model_name}' trouv√© et disponible")
            return True
        else:
            print(f"‚ö†Ô∏è  Mod√®le '{model_name}' non trouv√©")
            print(f"   Mod√®les disponibles: {', '.join(model_names)}")
            print(f"\n   Pour t√©l√©charger le mod√®le:")
            print(f"   ollama pull {model_name}")
            return False
    
    except Exception as e:
        print(f"‚ùå Erreur: {str(e)}")
        return False


def test_embedding_generation(text: str) -> bool:
    """
    Teste la g√©n√©ration d'un embedding.
    """
    print_header("Test 3: G√©n√©ration d'embedding")
    
    try:
        url = f"{OLLAMA_BASE_URL}/api/embeddings"
        payload = {
            "model": EMBEDDING_MODEL,
            "prompt": text
        }
        
        print(f"üìù Texte √† vectoriser: '{text}'")
        print(f"üîÑ G√©n√©ration en cours...")
        
        start_time = time.time()
        response = httpx.post(url, json=payload, timeout=30.0)
        response.raise_for_status()
        execution_time = (time.time() - start_time) * 1000
        
        data = response.json()
        embedding = data.get('embedding')
        
        if not embedding:
            print("‚ùå Aucun embedding retourn√©")
            return False
        
        print(f"‚úÖ Embedding g√©n√©r√© avec succ√®s")
        print(f"   - Dimensions: {len(embedding)}")
        print(f"   - Temps: {execution_time:.2f} ms")
        print(f"   - Premiers 5 √©l√©ments: {embedding[:5]}")
        
        return True
    
    except httpx.HTTPStatusError as e:
        print(f"‚ùå Erreur HTTP {e.response.status_code}")
        print(f"   R√©ponse: {e.response.text}")
        return False
    
    except Exception as e:
        print(f"‚ùå Erreur: {str(e)}")
        return False


def test_similarity_search():
    """
    Teste la recherche de similarit√© entre plusieurs embeddings.
    """
    print_header("Test 4: Recherche de similarit√©")
    
    texts = [
        "Django est un framework web Python",
        "Python est un langage de programmation",
        "Le chat dort sur le canap√©"
    ]
    
    try:
        embeddings = []
        
        print("üîÑ G√©n√©ration des embeddings...")
        for text in texts:
            url = f"{OLLAMA_BASE_URL}/api/embeddings"
            payload = {"model": EMBEDDING_MODEL, "prompt": text}
            response = httpx.post(url, json=payload, timeout=30.0)
            response.raise_for_status()
            embedding = response.json().get('embedding')
            embeddings.append(embedding)
            print(f"   ‚úì '{text[:40]}...'")
        
        # Calcul de la similarit√© cosinus
        import numpy as np
        
        def cosine_similarity(vec1, vec2):
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        
        print("\nüìä Matrice de similarit√©:")
        print("     ", end="")
        for i in range(len(texts)):
            print(f"  Text{i+1}", end="")
        print()
        
        for i, emb1 in enumerate(embeddings):
            print(f"Text{i+1}", end="")
            for j, emb2 in enumerate(embeddings):
                similarity = cosine_similarity(emb1, emb2)
                print(f"  {similarity:.3f}", end="")
            print()
        
        # Analyse
        sim_01 = cosine_similarity(embeddings[0], embeddings[1])
        sim_02 = cosine_similarity(embeddings[0], embeddings[2])
        
        print(f"\nüìà Analyse:")
        print(f"   - Text 1 vs Text 2 (Python/Django): {sim_01:.3f}")
        print(f"   - Text 1 vs Text 3 (Django/Chat):   {sim_02:.3f}")
        
        if sim_01 > sim_02:
            print(f"   ‚úÖ Coh√©rent: Les textes similaires ont un score plus √©lev√©")
            return True
        else:
            print(f"   ‚ö†Ô∏è  Inattendu: Les textes diff√©rents ont un score plus √©lev√©")
            return False
    
    except Exception as e:
        print(f"‚ùå Erreur: {str(e)}")
        return False


def main():
    """
    Ex√©cute tous les tests.
    """
    print("\n" + "üî¨ SMART-NOTEBOOK - Tests Ollama".center(60))
    
    results = []
    
    # Test 1: Connexion
    results.append(("Connexion Ollama", test_ollama_connection()))
    
    if not results[-1][1]:
        print("\n‚ùå Impossible de continuer sans connexion Ollama")
        sys.exit(1)
    
    # Test 2: Disponibilit√© du mod√®le
    results.append((f"Mod√®le {EMBEDDING_MODEL}", check_model_availability(EMBEDDING_MODEL)))
    
    if not results[-1][1]:
        print("\n‚ö†Ô∏è  Continuons quand m√™me avec les tests suivants...")
    
    # Test 3: G√©n√©ration d'embedding
    results.append(("G√©n√©ration d'embedding", test_embedding_generation("Hello, this is a test")))
    
    # Test 4: Similarit√© (n√©cessite numpy)
    try:
        import numpy
        results.append(("Recherche de similarit√©", test_similarity_search()))
    except ImportError:
        print("\n‚ö†Ô∏è  numpy n'est pas install√©, test de similarit√© ignor√©")
        results.append(("Recherche de similarit√©", None))
    
    # R√©sum√©
    print_header("R√âSUM√â DES TESTS")
    
    total = len([r for r in results if r[1] is not None])
    passed = len([r for r in results if r[1] is True])
    
    for name, result in results:
        if result is True:
            status = "‚úÖ PASS"
        elif result is False:
            status = "‚ùå FAIL"
        else:
            status = "‚è≠Ô∏è  SKIP"
        print(f"{status} - {name}")
    
    print(f"\nüìä Score: {passed}/{total} tests r√©ussis")
    
    if passed == total:
        print("\nüéâ Tous les tests sont pass√©s ! Ollama est pr√™t pour Smart-Notebook.")
        sys.exit(0)
    else:
        print("\n‚ö†Ô∏è  Certains tests ont √©chou√©. V√©rifiez la configuration.")
        sys.exit(1)


if __name__ == "__main__":
    main()
